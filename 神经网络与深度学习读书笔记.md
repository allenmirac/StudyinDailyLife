# 神经网络与深度学习读书笔记

多目标优化算法，时间序列模型 title topic

在粗略看完西瓜书之后，还是有太多地方不理解，又找到了这本书，记录一下学习心得。

实际代码，还要去看Dive into DL

```
摘自 邱锡鹏，神经网络与深度学习，机械工业出版社，https://nndl.github.io/, 2020.
```

## 前馈神经网络

“人生海海，潮起之后是潮落”

神经网络是一种典型的分布式并行处理模型，通过大量神经元之间的交互来处理信息，每一个神经元都发送兴奋和抑制的信息到其他神经元。与感知器不同，神经网络中使用的激活函数（Activation function）一般为连续可导函数。

![常见的激活函数及其导数](https://s2.loli.net/2024/10/03/KDBJ7Xmu8hqPLta.png)

前馈神经网络是一种类型最简单的网络，相邻两层的神经元之间为全连接关系，也称为全连接神经网络（Fully Connected Neural Network，FCNN）或多层感知器。

相当于向前推进，正向推进，

![多层前馈神经网络](https://s2.loli.net/2024/10/03/HsJpPhtkfbRvmao.png)

在这个神经网络中的参数主要通过梯度下降（Gradient Descent）来计算并优化，当确定了风险函数以及网络结构后，就可以使用链式法则来计算参数的梯度（导数）。自动计算梯度的方法可以分为以下三类：数值微分、符号微分和自动微分．

反向传播梯度计算简要过程：https://blog.csdn.net/weixin_55189321/article/details/131318167

## 卷积神经网络

“以中有足乐者，不知口体之奉不若人也”

卷积神经网络（Convolutional Neural Network，CNN 或ConvNet）是一种具有局部连接（对比全连接）、权重共享等特性的深层前馈神经网络．一般由卷积层（提取局部区域的特征，Feature Map，不同的卷积核相当于不同的特征提取器）、汇聚层（降低特征维数，减少参数数量）和全连接层构成。

全连接前馈神经网络存在一下两个主要问题：

（1）<font color='red'>参数太多</font>：意思就是维度过高，层数增加，参数规模会急剧增加，导致训练效率非常低，也容易出现过拟合。

（2）<font color='red'>局部不变性特征</font>：自然图像中的物体都具有局部不变性特征，比如尺度缩放、平移、旋转等操作不影响其语义信息．而全连接前馈网络很难提取这些局部不变性特征，一般需要进行数据增强来提高性能．

不理解一维卷积（一维矩阵）、二维卷积（二维矩阵）和卷积核，卷积操作后（加了一个矩阵）就可以得到特征映射（Feature Map）？！

此外，还引入了卷积的变种：加入卷积核的滑动步长和零填充，增加了卷积的多样性。看不懂

![卷积的步长和零填充](https://s2.loli.net/2024/10/03/1QZ9lLtyawBiS3O.png)

AlexNet是第一个现代深度卷积神经网络，网络结构如下图所示，包括5个卷积层、三个汇聚层、和三个全连接层，最后一层使用的是SoftMax函数的输出层，使用ReLu函数作为非线性激活函数，获得了2012年ImageNet的图像分类的冠军。

![AlexNet网络结构](https://s2.loli.net/2024/10/05/MoA4Y5F1ucfZqVO.png)

典型的网络还包括：Inception 网络、GoogLeNet。

**残差网络**（Residual Network，ResNet）通过给非线性的卷积层增加直连边（Shortcut Connection）（也称为残差连接（Residual Connection））的方式来提高信息的传播效率

如果将目标函数拆分成两部分：恒等函数（Identity Function）𝒙 和残差函数（Residue Function）ℎ(𝒙) − 𝒙．

![目标函数](https://s2.loli.net/2024/10/17/D48laRdN5xKXzce.png)

根据通用近似定理，一个由神经网络构成的非线性单元有足够的能力来近似逼近原始目标函数或残差函数，但实际中后者更容易学习。

![一个简单的残差单元结构](https://s2.loli.net/2024/10/17/vImY4WOQTeS1orA.png)

残差单元由多个级联的（等宽）卷积层和一个跨层的直连边组成，再经过ReLU 激活后得到输出。残差网络就是将很多个残差单元串联起来构成的一个非常深的网络。

把卷积理解为采样？

## 循环神经网络

看不下去了/(ㄒoㄒ)/~~

**循环神经网络**（Recurrent Neural Networks，RNN）是一类专门用于处理序列数据的神经网络。与传统的前馈神经网络不同，RNN具有内部的循环结构，可以在序列数据的时间步之间传播信息。这使得RNN能够记住序列中的先前输入，适合处理时间序列、自然语言处理等需要捕捉时间依赖关系的任务。

### RNN的基本特点

1. **时间步之间的信息共享**：RNN通过隐状态（Hidden State）在不同的时间步之间传递信息。这意味着当前的输出不仅依赖于当前的输入，还依赖于之前的输入。
2. **权重共享**：在每个时间步，RNN的网络权重是共享的，这使得它能够处理任意长度的序列。
3. **梯度消失和爆炸问题**：由于RNN在反向传播时需要通过许多时间步的累积梯度，可能会遇到梯度消失或爆炸的问题。为了解决这一问题，改进的RNN结构如LSTM（长短时记忆网络）和GRU（门控循环单元）被提出。

### RNN的应用

- **自然语言处理**：RNN被广泛应用于文本生成、机器翻译、语言模型等。
- **时间序列预测**：可以用RNN预测股票价格、天气等时间序列数据。
- **语音识别**：RNN在连续语音的识别和处理上也有很好的表现。

![循环神经网络](https://s2.loli.net/2024/10/17/D93afxLprtiK4BM.png)

隐藏层的活性值𝒉𝑡 在很多文献上也称为状态（State）或隐状态（Hidden State）．

### RNN的参数学习

循环神经网络中存在一个递归调用的函数𝑓(⋅)，因此其计算参数梯度的方式和前馈神经网络不太相同．在循环神经网络中主要有两种计算梯度的方式：**随时间反向传播（BPTT）算法和实时循环学习（RTRL）算法**．

BPTT是标准的反向传播算法的扩展，它适用于处理序列数据的模型，如RNN。由于RNN具有时间步之间的依赖，BPTT通过在时间序列的多个时间步上展开网络，将时间维度考虑进来。

参数学习的主要过程

1. **前向传播**： 在每个时间步，RNN接受输入，计算隐状态，生成输出。隐状态在不同的时间步之间传递，并且通过同一组权重矩阵进行计算。

   - 输入 $$x_t$$（时间步 t 的输入）
   - 隐状态 $$h_t$$（时间步 t 的隐藏层状态）
   - 输出 $$y_t$$（时间步 t 的输出）

   计算公式：

   $$ht=f(W_{xh}x_t+W_{hh}h_{t−1})$$

   $$yt=g(W_{hy}y_t)$$

   其中，$$W_{xh}, W_{hh}, W_{hy}$$ 分别是输入到隐藏层、隐藏层到隐藏层、隐藏层到输出层的权重矩阵，f 和 g 是激活函数。

2. **损失函数计算**： 根据网络的输出 $$y_t$$ 和目标输出$$\hat{y}_t$$ 计算损失 $$L_t$$。总损失是所有时间步损失的总和或平均：

   $$L=\sum_{t=1}^{T}L_t$$

   其中 T 是序列的长度。

3. **反向传播通过时间（BPTT）**： 在反向传播过程中，RNN展开成一个长的前馈网络，时间步的数量等于序列长度。BPTT通过这种展开网络，计算每个时间步的梯度。

   - 由于 $$h_t$$ 的计算依赖于之前的$$h_{t-1}$$，梯度需要通过时间回传。
   - 计算从损失函数到每个权重的梯度，并更新权重。

   BPTT步骤：

   - 计算每个时间步的梯度，首先从最后一个时间步（比如时间步 T）开始计算： $$\frac{\partial L}{\partial h_T}, \frac{\partial L}{\partial W_{xh}}, \frac{\partial L}{\partial W_{hh}}, \frac{\partial L}{\partial W_{hy}}$$
   - 随着时间步向前传播，逐步累积梯度，直至到达第一个时间步 t=1。

4. **权重更新**： 使用梯度下降法等优化算法更新权重：

   $$W=W - \alpha \frac{\partial L}{\partial W}$$

   其中，α 是学习率，$$W$$ 代表不同的权重矩阵。

**梯度消失和梯度爆炸问题**

在RNN中，由于时间步之间的依赖性，反向传播时的梯度可能会随着时间的增加或减少而出现以下问题：

1. **梯度消失**：在长序列中，梯度在多个时间步之间逐步缩小，导致早期时间步的权重几乎无法更新。特别是在使用像sigmoid或tanh这种激活函数时，梯度容易衰减。
2. **梯度爆炸**：反之，如果梯度在多个时间步中迅速增大，权重会发生不稳定的大幅更新，导致训练过程发散。

为了解决这些问题，提出了几种改进方法：

- **梯度裁剪**：限制梯度的最大值，使得过大的梯度不会引发爆炸问题。
- **LSTM（长短时记忆网络）和GRU（门控循环单元）**：这两种网
- 络通过设计特殊的门控机制（如输入门、遗忘门、输出门等）来控制信息流动，有效缓解了梯度消失问题。

### 长程依赖问题

循环神经网络（RNN，Recurrent Neural Networks）在处理序列数据时表现出了很大的优势，能够捕捉数据的时间依赖性。然而，RNN的一个重要缺陷是**长程依赖问题**，即在处理长序列时，RNN难以有效捕捉到序列中相隔较远的依赖关系。

**具体原因**

RNN通过逐步更新隐藏状态来处理序列信息，每个时间步的输入和隐藏状态都会影响后续的状态。然而，当序列很长时，RNN中的梯度在反向传播过程中可能会出现以下两种情况：

1. **梯度消失**：远处时间步的误差信息经过多次梯度传播后逐渐变得非常小，以至于无法对早期的时间步进行有效的权重更新。这使得网络难以学习长时间跨度的依赖关系。
2. **梯度爆炸**：相反的，梯度值可能变得非常大，导致权重更新过度，模型不稳定。

这两种问题导致RNN在处理长距离依赖时表现不佳，通常只能捕捉相邻时间步之间的短期依赖，而忽略了序列中远距离的依赖关系。

**解决方案**

为了解决RNN的长程依赖问题，研究人员提出了几种改进方法，最常见的有以下两种：

1. **长短期记忆网络（LSTM）**： LSTM通过引入“遗忘门”、“输入门”和“输出门”来控制信息的流动，从而更好地保留长时间步的信息。LSTM能够选择性地记住或忘记过去的信息，因此在长序列依赖问题上表现更好。
2. **门控循环单元（GRU）**： GRU是LSTM的简化版本，合并了一些门控机制，使得其结构更加简单，同时能够在大多数情况下达到与LSTM相似的效果。

### 基于门控的循环神经网络

长短期记忆网络（Long Short-Term Memory Network，LSTM）是循环神经网络的一个变体，可以有效地解决简单循环神经网络的梯度爆炸或消失问题．，LSTM 网络引入门控机制（Gating Mechanism）来控制信息传递的路径．

**什么是记忆？**

循环神经网络中的隐状态𝒉 存储了历史信息，可以看作一种记忆（Memory）在简单循环网络中，隐状态每个时刻都会被重写，因此可以看作一种短期记忆（Short-Term Memory）．在神经网络中，长期记忆（Long-Term Memory）可以看作网络参数，隐含了从训练数据中学到的经验，其更新周期要远远慢于短期记忆．而在LSTM 网络中，记忆单元𝒄 可以在某个时刻捕捉到某个关键信息，并有能力将此关键信息保存一定的时间间隔．记忆单元𝒄 中保存信息的生命周期要长于短期记忆𝒉，但又远远短于长期记忆，因此称为长短期记忆（LongShort-Term Memory）．

长短期记忆是指长的“短期记忆”．

### 深层循环神经网络

增加循环神经网络的深度从而增强循环神经网络的能力，

有两种常见的做法：

（1）堆叠循环神经网络：一种常见的增加循环神经网络深度的做法是将多个循环网络堆叠起来，称为堆叠循环神经网络（Stacked Recurrent Neural Network，SRNN）．一个堆叠的简单循环网络（Stacked SRN）也称为循环多层感知器（Recurrent Multi-Layer Perceptron，RMLP）
![堆叠循环神经网络](https://s2.loli.net/2024/10/23/zJLEGscaQI2yxwb.png)

（2）双向循环神经网络：双向循环神经网络（Bidirectional Recurrent Neural Network，Bi-RNN）由
两层循环神经网络组成，它们的输入相同，只是信息传递的方向不同。一个时刻的输出不仅与过去时刻的信息有关、也和后续时刻的信息有关。
![双向循环神经网络](https://s2.loli.net/2024/10/23/vXg12GhVJC7jbzn.png)

## 深度信念网络

深度信念网络（Deep Belief Network，DBN）是一种深层的概率有向图模型，其图结构由多层的节点构成，每层节点的内部没有连接，相邻两层的节点之间为全连接

![一个有4 层结构的深度信念网络](https://s2.loli.net/2024/10/23/Qdhn8ZEP3R2Ox71.png)

深度信念网络是一个生成模型，可以用来生成符合特定分布的样本．隐变量用来描述在可观测变量之间的高阶相关性。

它的的训练过程可以分为**逐层预训练**和**精调**两个阶段．先通过逐层预训练将模型的参数初始化为较优的值，再通过传统学习方法对参数进行精调．

## 变分自编码器

变分自编码器（Variational Autoencoder, VAE）是一种生成模型，属于深度学习的一种类型。它结合了概率和神经网络，通过对数据的潜在分布建模，从而生成与训练数据相似的新数据。VAE的基本思想是学习数据的“潜在空间”表示，这种表示可以用来生成相似的数据点。与传统自编码器不同，VAE是基于概率模型的，并引入了随机性来增强生成数据的多样性。 

VAE的主要组成部分包括编码器、解码器和采样过程：

1. **编码器（Encoder）：**编码器将输入数据映射到一个潜在空间，并输出潜在空间中的均值和方差，从而形成一个多维高斯分布。这个分布允许我们对数据的潜在表示进行采样，而不是直接得到一个确定的表示。

2. **采样（Sampling）：**在潜在空间中，从上一步得到的高斯分布中随机采样一个向量。这一采样步骤引入了随机性，使得VAE可以生成多样化的样本。

3. **解码器（Decoder）：**解码器将采样得到的潜在向量映射回原始数据空间，从而生成与输入数据类似的数据。

### VAE的损失函数
VAE的训练目标是最小化两个损失项：

- **重构损失（Reconstruction Loss）：**确保解码器的输出与输入数据相似，使生成的样本尽可能接近真实数据。
- **KL散度（Kullback-Leibler Divergence）：**使潜在分布接近标准正态分布，从而避免潜在空间过于分散，使生成的样本具有一致性和多样性。

这种损失函数的设计允许VAE不仅可以有效地重建输入数据，还可以生成新的数据样本。
