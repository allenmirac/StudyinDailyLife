# 深度学习数学基础

万丈高楼平地起

怎么说呢，学的数二对于这些东西还是太陌生了，而且当时学的只会做题，不知道怎么使用/(ㄒoㄒ)/~~

所以记下来一些不太清楚的前置知识点，主要来自《艾伯特深度学习》，书中内容很多，和《邱锡鹏神经网络与深度学习》内容有点相似


## 牛顿-莱布尼茨公式

揭示了定积分与被积函数的原函数或者不定积分之间的联系。 

![牛顿莱布尼茨公式](https://bkimg.cdn.bcebos.com/formula/0ee1ed05e107b3c9f50f9bb76c547afa.svg)

## 泰勒公式

以直代曲，一点一世界，数值越大高阶的作用越大，但是高阶的作用有时候过大了，所以采用阶乘来抵消一些作用，最终得到的效果就是更好的模拟曲线。

![泰勒展开式](https://bkimg.cdn.bcebos.com/formula/8e916fa2c76aef2f90a408d6d3206eaa.svg)

## 拉格朗日乘数法（乘子法）

求在约束条件下的极值。

## 方向导数

fai 的角度大小要注意

![方向导数](https://s2.loli.net/2024/09/27/92wdcBpTOhgmUDV.png)

## 梯度

蚂蚁跑离有火的地方，沿着梯度方向跑。梯度是一个向量，它的方向与方向导数最大值取得的方向一致。

## 概率与信息论的作用

概率论是用于表明不确定性申明的数学框架，提供的一种**量化不确定性**的方法，也提供了用于**导出新的不确定性声明**的公理。

概率可以被看作是用于处理不确定性的逻辑扩展。逻辑提供了一套形式化的规则，可以在给定某些命题是真或假的假设下，判断另外一些命题是真的还是假的。概率论提供了一套形式化的规则，可以在给定一些命题的似然后，计算其他命题为真的似然。

机器学习很多情况下需要处理不确定量，有时也需要处理随机量，随机性和不确定性来自于多方面，如建模系统内在的随机性、不完全观测、不完全建模等。

## 边缘概率

有时候，我们知道了一组变量的联合概率分布，但想要了解其中一个子集的概率分布。这种定义在子集上的概率分布被称为边缘概率分布(marginal probability distribution）

## 协方差和标准差

## 常用概率分布

## 信息论

主要研究的是对一个信号携带信息量的多少进行量化。

## 方差

偏离期望值的程度的大小

![方差计算公式](https://bkimg.cdn.bcebos.com/formula/4b63ab2e13bc1cd3aaa328321b185bd1.svg)

## 大数定理

重复实验的次数越多，随机事件的概率越接近于它的概率。

## 马尔可夫不等式

将概率与期望建立联系，

![马尔可夫不等式](https://wikimedia.org/api/rest_v1/media/math/render/svg/f4a5da8aeb446a9b74db9922fdde8568bcef2276)

## 范数

衡量向量的大小。当p = 2 时，L2 范数被称为欧几里得范数（Euclidean norm）。它表示从原点出发到向量x 确定的点的欧几里得距离。L2 范数在机器学习中出现地十分频繁。

![范数的定义](https://s2.loli.net/2024/09/14/CRLe8gSV79cAKjI.png)

## 特征值和特征向量

找特征向量的过程就是在找是否有一个矩阵左乘这个向量可以让它拉伸或者压缩。特征分解(eigendecomposition)是使用最广的矩阵分解之一，即我们将矩阵分解成一组特征向量和特征值。方阵A的特征向量(eigenvector)是指与A相乘后相当于对该向量进行缩放的非零向量v:![特征分解](https://bkimg.cdn.bcebos.com/formula/4973e26a6f4a367c9338895ce4471010.svg)，只有方阵才可以特征分解。

## 奇异值分解

还有另一种分解矩阵的方法，被称为奇异值分解(singular value decomposition,SVD),将矩阵分解为奇异向量(singular vector)和奇异值(singular value)。通过奇异值分解，我们会得到一些与特征分解相同类型的信息。

## 线代实例：主成分分析

PCA（principle components analysis），一个机器学习算法，可以通过线代的知识推导。见艾伯特深度学习2.12

# 西瓜书书本内容杂谈
把圈子变小，把语速放缓，把心放宽，把生活打理好

只能说快速过了一遍，花了一个多星期吧，然后后边的内容是一点也看不懂了（能发现前面记得比较详细，到了后边是看不懂一点了，脑壳痛QWQ

就不像是一本新手的入门书籍，太难受了ಥ_ಥ，了解概念这本书也不适合，还是转为看《邱锡鹏，神经网络与深度学习》和李沐动手学AI课的实践部分，ಠ_ಠ

## 第一章 绪论

1、预预测的值是离散值，这类学习任务叫做“分类”，如“好瓜”，“坏瓜”；若预预测的值是连续值，这类学习任务称为“回归”。

2、学得模型适用于新样本的能力，叫做“泛化”，泛化能力好，能更好的适用新样本。

3、西瓜问题的假设空间自顶向下、自底向上。

4、归纳偏好，对假设空间进行选择的启发式偏好，用一般性的原则来引导偏好，如”奥卡姆剃刀“法（Occam‘s razor）是一种常用的、自然科学研究中最基本的原则，即“若有多个假设与观察一致，则选择最简单的那个”，选择更为平滑的曲线。

5、不同算法针对不同问题有不同的拟合性，尽管看起来很差的算法（笨拙的算法）也有非常拟合的数据，。

6、所有算法的期望性能都差不多，E表示期望，下表ote表示训练集外误差（Out of Training set Error），E<sub>ote</sub>(L<sub>a</sub>|X,f)表示的就是给定数据集和真实目标函数的情况下，算法L<sub>a</sub>的训练集外误差的计算方式。（公式真的好难啊，脑壳痛QWQ）

![学习算法的总误差](https://s2.loli.net/2024/09/07/UArh6fMqCBjR8tw.png)

7、NFL定理（<font color=red>No Free Lunch</font>），针对具体的问题谈论算法的优劣，裁缝做衣服

![NFL定理](https://s2.loli.net/2024/09/07/FblyvzxMQLVcs6A.png)

8、历史发展过程，逻辑理论推理-->"知识期"(人为的去教电脑知识)-->自主学习知识，二十世纪八十年代是机器学习成为一个独立的学科领域、各种机器学习技术百花齐放。跳棋程序的发展历史：https://blog.creaders.net/u/5477/202405/487515.html

9、一些闪光的思想，“迁移学习”（Transfer Learning），“类比学习”（Learning By Analogy），“深度学习”（Deeping Learning）

10、课后习题看的头疼，完全看不懂，习题1.2直接可以用代码来解决了，厉害

11、机器学习的别称，萨缪尔（研制了一个西洋跳棋程序）将其定义为“<font color=red>不显示编程赋予计算机能力的研究领域</font>”。

## 第二章 模型评估与选择

1. 术语：误差（error），训练误差（training error）和经验误差（empirical error）是指学习器在训练集上得到的误差；泛化误差（generalization error）；过拟合（overfitting）是当学习器把训练样本学得“太好”了的时候，很可能已经把训练样本自身的一些特点当作了所有潜在样本都当成了具有的一般性质；相对的是欠拟合（underfitting）。
2. 进行模型评估时，需要使用特定的方法来实现训练集S和测试集T的划分，并且训练集与测试集尽可能互斥。模型评估是通过实验测试来对学习器的泛化误差进行评估，在现实任务中往往还会考虑时间开销、存储开销、可解释性等方面的因素，这里暂且只考虑泛化误差。先交叉验证集选择模型，然后训练集训练模型，最后测试集评估模型
3. 评估方法：评估方法不能理解，第二次看有了下面的一些理解。
   **训练集（Training Set）**：用于**训练**模型。

   **验证集（Validation Set）**：用于**调整和选择**模型。

   **测试集（Test Set）**：用于**评估**最终的模型。

   当我们拿到数据之后，一般来说，我们把数据分成这样的三份：训练集（60%），验证集（20%），测试集（20%）。用训练集训练出模型，然后用验证集验证模型，根据情况不断调整模型，选出其中最好的模型，记录最好的模型的各项选择，然后据此再用（训练集+验证集）数据训练出一个新模型，作为最终的模型，最后用测试集<font color='red'>评估</font>最终的模型。
   1. **留一法（Leave One Out Cross Validation，LOOCV）**：m个样本集合，拿出一个作为验证集，剩余m-1个作为训练集，这样进行m次当都的训练和验证，最后将m次验证结果取平均值，作为验证误差。**缺点是计算量大，一般不作为实际使用**
   2. **K折交叉验证法（K-Fold Cross Validation）**：把数据集分成K份，每个子集互不相交且大小相同，依次从K份中选出1份作为验证集，其余K-1份作为训练集，这样进行K次单独的模型训练和验证，最后将K次验证结果取平均值，作为此模型的验证误差。当K=m时，就变为留一法。可见留一法是K折交叉验证的**特例**。根据经验，**K一般取10**。（在各种真实数据集上进行实验发现，10折交叉验证在偏差和方差之间取得了最佳的平衡。）
   3. **多次K折交叉验证（Repeated K-Fold Cross Validation）**：每次用不同的划分方式划分数据集，每次划分完后的其他步骤和K折交叉验证一样。例如：10 次 10 折交叉验证，即每次进行10次模型训练和验证，这样一共做10次，也就是总共做100次模型训练和验证，最后将结果平均。这样做的目的是让结果更精确一些。（研究发现，重复K折交叉验证可以提高模型评估的精确度，同时保持较小的偏差。）
6. 性能度量：
   1. 错误率与精度
   2. 查准率P与查全率R：一组矛盾的度量，以P为纵轴，R为横轴，得到的曲线为P-R曲线图平衡点（Break Even Point）是查准率=查全率，查准率和查全率是**一对“鱼”与“熊掌”**，一把来说，查准率高时，查全率往往偏低；而查全率高时，查准率往往偏低。
   
       例如，如果想将垃圾邮件都选取出来，可以将所有邮件都标签为垃圾邮件，那么查全率就接近于1，但这样查准率就会比较低；如果希望分类垃圾邮件的查准率足够高，那么可以让分类器尽可能挑选最有把握的垃圾邮件，但这样往往会有大量的垃圾邮件成为漏网之鱼，此时查全率就会比较低。
   
   3. F1度量：F1是基于查准率与查全率的调和平均(harmonic mean)定义的，能够表达对于查准率和查全率 的不同偏好。
       ![P-R曲线图](https://s2.loli.net/2024/09/12/R6CYgjEoa2yBhqF.png)
       ![F1度量](https://i-blog.csdnimg.cn/blog_migrate/8f2cacd1d19ca535ba447d751a1e9fb1.jpeg)
   4. 当β>0时，度量了查全率和查准率的相对重要性。β=1，退化为标准的F1
   5. 二分类问题得出的分类结果可以得出“混淆矩阵”。
   6. ROC曲线和AUC（度量分类中的非均衡性），
   7. ROC 关注两个指标：
   
       TPR（True Positive Rate）表示在所有实际为正例（阳性）的样本中，被正确地判断为正例的比率，即：
   
           TPR=TP/(TP + FN)
   
       FPR（ False Positive Rate）表示在所有实际为反例（阴性）的样本中，被错误地判断为正例的比率，即：
   
           FPR=FP/(FP + TN）
   
       ***\*AUC值\****（Area Unser the Curve）是ROC曲线下的面积
   

![ROC曲线](https://i-blog.csdnimg.cn/blog_migrate/c6aeafc9bf3e9bb77a73427f37cd4e1b.jpeg)

## 第三章 线性模型

1. 线性回归（linear regression）：试图完成一条直线f(x)=wx+b，并且使得f(x)接近y，如何衡量他们之间的差距，采用均方误差的方式（最常用的性能度量），在线性回归中，基于均方误差最小化来进行模型求解的方法叫做“最小二乘法”，最小二乘法就是试图找到一条直线，使所有样本到直线的欧式距离之和最小。对应的，多元线性回归（multivariate linear regression）就是对应于多个w和多个x，以向量形式来处理，
2. 对数几率回归，使用Sigmoid激活函数

## 第四章 决策树

Decision Tree是一种解决分类问题的算法，监督学习主要有两种任务：分类（连续）、回归（离散）。

决策树算法采用树结构，层层推理来实现最终的分类。

决策树学习的三个步骤：特征选择、决策树生成、决策树剪枝（解决过拟合问题，随机森林很大程度减少过拟合）。

三种典型的决策树算法：ID3（采用信息增益）、C4.5（改进、采用信息增益比）、CART（采用基尼系数，**CART树全称Classification And Regression Trees**）

**关于信息增益的理解：**

熵：表示随机变量的不确定性

条件熵：在一个条件下、随机变量的不确定性能

信息增益：熵-条件熵，表示是在一个条件下，信息不确定性减少的程度。

详细解释：[信息增益到底怎么理解呢？](https://www.zhihu.com/question/22104055)

剪枝处理，剪枝是决策树学习算法用于对付过拟合的主要手段，基本策略包括预剪枝、后剪枝。这里过拟合是指分支过多，训练结果“太好了”

## 第五章 神经网络

神经网络是一种模仿人脑神经系统的数学模型，称为人工神经网络，简称神经网络．在机器学习领域，神经网络是指由很多人工神经元构成的网络结构模型，这些人工神经元之间的连接强度是可学习的参数．

人脑神经元结构，典型的神经元结构可以分为细胞体和细胞突起：
（1） 细胞体（Soma）中的神经细胞膜上有各种受体和离子通道，胞膜的受体可与相应的化学物质神经递质结合，引起离子通透性及膜内外电位差发生改变，产生相应的生理活动：兴奋或抑制．
（2） 细胞突起是由细胞体延伸出来的细长部分，又可分为树突和轴突．
a ) 树突（Dendrite）可以接收刺激并将兴奋传入细胞体．每个神经元可以有一或多个树突．
b ) 轴突（Axon）可以把自身的兴奋状态从胞体传送到另一个神经元或其他组织．每个神经元只有一个轴突．

![典型的神经元结构](https://s2.loli.net/2024/10/02/oRNp8IiHZY2jaQh.png)

人工神经网络是为模拟人脑神经网络而设计的一种计算模型，它从结构、实现机理和功能上模拟人脑神经网络．人工神经网络与生物神经元类似，由多个节点（人工神经元）互相连接而成，可以用来对数据之间的复杂关系进行建模。

神经网络的不同节点的连接被赋予了不同的权重，每个权重代表了一个节点对另一个节点的影响大小，经过权重综合计算，将其输入到一个激活函数中得到一个<font color='red'>新的活性值（兴奋或抑制）</font>。

![典型的神经元结构](https://s2.loli.net/2024/10/02/YsZC9IbMovPrG5d.png)

参考：邱锡鹏，神经网络与深度学习，机械工业出版社，https://nndl.github.io/, 2020.



## 第六章 支持向量机

支持向量机（Support Vector Machine，SVM）是一个经典的二分类算法，其找到的分割超平面具有更好的鲁棒性。

首先定义间隔γ表示整个数据集地所有样本到分割超平面地最小距离，γ越大，其分割超平面对两个数据集地划分就越稳定，不容易收到噪声地干扰。支持向量机的目标就是<font color='red'>**寻找一个超平面使得γ最大**</font>。

![SVM示例](https://s2.loli.net/2024/10/03/ucNXTIil9OGFsx7.png)

摘自 邱锡鹏，神经网络与深度学习，机械工业出版社，https://nndl.github.io/, 2020.

## 第七章 贝叶斯分类器

贝叶斯分类是一类分类算法的总称，贝叶斯算法是这类算法的核心，

**极大似然估计（Maximum Likelihood Estimation，MLE）和贝叶斯估计（Bayesian Estimation）是统计推断中两种最常用的参数估计方法**，这两种方法，也分别是频率派和贝叶斯派的观点

机器学习所要实现的是基于有限的训练样本尽可能准确的估计后验概率p(c|x)，有两种方式来获取这个概率：

1. 判别式模型，通过直接建模来获取后验概率
2. 生成式模型，通过联合概率模型来建模，然后再获得后验概率，贝叶斯方法就是基于这个模型

贝叶斯理论模型的**最优**性，选择后验概率中最大的那一个作为预测结果

**分类：**朴素贝叶斯分类器、半朴素贝叶斯分类器（独依赖估计、TAN、AODE）、贝叶斯网。

通俗来讲，通过先验概率的计算后来获得后验概率的过程，其实就是通过已知的经验来判断后来的事情，经验越多，预测得到的东西也就越准。

这种分类方式有一定的缺点，根本原因是，在决策过程中假设太强了，而且可能面临维度灾难，要考虑的特征过多。

[8种异常检测算法](https://blog.csdn.net/m0_59596937/article/details/128877355)

课后小故事，贝叶斯怎么有这么多谜团，这也能出名，他从事数学研究的目的是为了证明上帝的存在（笑。

## 第八章 集成学习

粗略看了一下

[集成学习](https://easyai.tech/ai-definition/ensemble-learning/)

## 第九章 聚类

这是一种非监督学习的算法（与classification最大的区别），将不同性质的数据分成几个相同类型的数据，如何评定相同类型的数据，可以有下面这些标准：Euclidean distance, Cosine similarity, Manhattan distance, etc.

聚类算法的分类：

1. Centroid-based Clustering (Partitioning methods)
2. Density-based Clustering (Model-based methods)
3. Connectivity-based Clustering (Hierarchical clustering)
4. Distribution-based Clustering

[Clustering in Machine Learning - GeeksforGeeks](https://www.geeksforgeeks.org/clustering-in-machine-learning/#what-is-clustering-)

## 第十章 降维与度量学习



[第十一讲降维与度量学习](https://ins.sjtu.edu.cn/people/xuzhiqin/2020opti/2020machinelearning/%E7%BB%9F%E8%AE%A1%E8%AE%A1%E7%AE%97%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AD%A6%E7%94%9F%E6%95%B4%E7%90%86%E7%9A%84%E8%AE%B2%E4%B9%89/%E7%AC%AC%E5%8D%81%E4%B8%80%E8%AE%B2%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0.pdf)

## 第十一章 特征选择与稀疏学习

在机器学习中特征选择是一个重要的“**数据预处理**”（**data** **preprocessing**）过程，即试图从数据集的所有特征中挑选出与当前学习任务相关的特征子集，接着再利用数据子集来训练学习器；稀疏学习则是围绕着稀疏矩阵的优良性质，来完成相应的学习任务。

特征选择一般在获得数据之后首先需要进行的，因为在实际任务中经常会遇到维数灾难问题，其中提取的特征过多，大多数可能都是冗余的，所以要提取出重要特征来减少维数。

[周志华《Machine Learning》学习笔记(13)--特征选择与稀疏学习](https://github.com/Vay-keen/Machine-learning-learning-notes/blob/master/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(13)--%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E4%B8%8E%E7%A8%80%E7%96%8F%E5%AD%A6%E4%B9%A0.md)

## 第十二章 计算学习理论

顾名思义，Computational learning theory研究的是关于通过计算来进行学习的理论，分析学习人物的困难本质，为学习算法提供理论保证，并根据分析结果指导算法设计。

PAC（Probably Approximately Correct，<font color='red'>可能近似正确</font>） 学习可以分为两部分：

1. 近似正确（Approximately Correct）：泛化错误小于一个界限，一般为1/2，即，0 < 𝜖 <1/2；机器学习中一个很关键的问题是期望错误和经验错误之间的差异，称为泛化错误（Generalization Error）．泛化错误可以衡量一个机器学习模型𝑓 是否可以很好地泛化到未知数据．
2. 可能（Probably）：一个学习算法<font color='red'>可能</font>以一定的概率学习到这样一个近似正确的假设。

## 第十三章 半监督学习

在许多ML的实际应用中，很容易找到海量的无类标签的样例，但需要使用特殊设备或经过昂贵且用时非常长的实验过程进行人工标记才能得到有类标签的样本，由此产生了极少量的有类标签的样本和过剩的无类标签的样例。因此，人们尝试将大量的无类标签的样例加入到有限的有类标签的样本中一起训练来进行学习，期望能对学习性能起到改进的作用，由此产生了半监督学习(Semi-supervised Learning)。

在做半监督学习(Semi-supervised Learning)的时候通常的情景如下：

1. unlabeled data的数量要远大于label data
2. 直推半监督学习(Semi-supervised Learning)只处理样本空间内给定的训练数据，利用训练数据中有类标签的样本和无类标签的样例进行训练，预测训练数据中无类标签的样例的类标签
3. 归纳半监督学习(Semi-supervised Learning)处理整个样本空间中所有给定和未知的样例，同时利用训练数据中有类标签的样本和无类标签的样例，以及未知的测试样例一起进行训练，不仅预测训练数据中无类标签的样例的类标签，更主要的是预测未知的测试样例的类标签。

半监督学习的需求非常强烈，因为在现实应用中往往能够很容易的收集到大量没有标记的样本。

## 第十四章 概率图模型

概率图模型（probabilistic graphical model, PGM），是一种学习任务的框架描述，它将学习任务归结为计算变量的概率分布，巧妙的结合了图论和概率论。

按照概率图中变量关系的不同，概率图模型可以大致分为两类：

- 贝叶斯网络：有向图模型，使用有向无环图表达关系（通常，变量间存在显式的因果关系）
- 马尔科夫网络：无向图模型，使用无图表达关系（通常，变量间存有关系，但是难以显式表达）

[概率图模型总览](https://longaspire.github.io/blog/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B%E6%80%BB%E8%A7%88)

## 第十五章 规则学习

机器学习中的“**规则**”(rule)通常是指语义明确、能描述数据分布所隐含的客观规律或领域概念

目标是产生一个能覆盖尽可能多的样例的规则集。

规则集生成的过程是一个贪心搜索的过程，因此为缓解过拟合的风险，最常见的做法就是剪枝（pruning）。

[规则学习](https://www.cnblogs.com/caolanying/p/16648584.html)

## 第十六章 强化学习

与监督学习不同的是，强化学习不需要带标签的输入输出对，同时也无需对非最优解的精确地纠正。其关注点在于寻找探索（对未知领域的）和利用（对已有知识的）的平衡。在一个学习过程结束后，根据获得的“奖励”来学习，并且进一步预测下面的东西。

# 吴恩达机器学习

写的比较乱，对于一些不懂的问题都记录在下面了

一个人在不接触对方的情况下，通过一种特殊的方式，和对方进行一系列的问答．如果在相当长时间内，他无法根据这些问题判断对方是人还是计算机，那么就可以认为这个计算机是智能的．
																																												——阿兰·图灵（Alan Turing）
																																						《Computing Machinery and Intelligence 》

![人工智能发展史](https://s2.loli.net/2024/10/02/ypk341ajhPZQoCU.png)

ps: 摘自 邱锡鹏，神经网络与深度学习，机械工业出版社，https://nndl.github.io/, 2020. P5

## 代价函数

cost function，所有样本误差的平均，与损失函数不同的地方是，loss function是定义在单个样本上的误差。

常用的是<font color=red>平方误差代价函数</font>

这里解释什么是代价，代价就是预测值与实际值之间的差距，对于多个样本来说就是代价之和，但是对于怎么处理这个和，有一定的门道，需要解决**正负值、数据量**与cost之间的关系，最终才采取了下面的函数

![代价函数](https://s2.loli.net/2024/09/15/UaRFvuoTnki7P9G.png)

其中平方项可以抵消误差值正负数带来的影响，m是样本量，建立了代价（cost）与样本量之间的关系；1/2是为了在对w,b求偏导（partial derivative）可以简化式子。

https://www.cnblogs.com/geaozhang/p/11442343.html

## 判断梯度下降是否收敛

观察目标函数的损失函数值的变化情况，

1. 梯度的范数：当梯度的范数趋近于零时，可以认为算法已经收敛。设置一个阈值。
2. 目标函数值的变化
3. 迭代次数，设置最大迭代次数
4. 收敛曲线图，看图说话

## 激活函数

理解不了为什么要将输出变成非线性的，如何理解为什么需要激活函数？如果你串联几个线性变换，得到的最后依旧是一个线性变换，例如，f(x)=3x+1, g(x)=2x+2,串联这两个函数得到的是一个f(g(x))=6x+7，因此，如果在层之间没有一些非线性，那么即使是深层堆叠也等效于单层，你无法用它解决非常复杂的问题。相反，具有非线性激活的足够大的 DNN 在理论上可以逼近任何连续函数。（这种解释太帅啦，但是还是不能理解串联起来有什么用处，各个神经层次之间为什么需要串联以及串联和求导是什么关系）

1. Sigmoid激活函数，也叫S形函数
   ![Sigmoid](https://bkimg.cdn.bcebos.com/formula/c652f9ceaee0518a93007ca80c51c934.svg)
2. ReLu函数
   ![img](https://bkimg.cdn.bcebos.com/formula/061b8705cd0dd363c1752fe0d9db0faa.svg)

3. Tanh函数
   ![img](https://bkimg.cdn.bcebos.com/formula/27dc16c40e8ca243c251fe1048fe68a9.svg)

Blog：https://www.cnblogs.com/XDU-Lakers/p/10557496.html

## 设置学习率

学习率Alpha，就是设置梯度下降的每一步的速率，不能过快也不能过慢

## 逻辑回归

没看懂

为了使反向传播正常工作，Rumelhart 和他的同事对 MLP 的架构进行了关键更改：他们用逻辑函数替换了阶跃函数，*σ*(*z*) = 1 / (1 + exp(–*z*))，也称为 S 形函数。

## 反向传播的工作原理

在 1970 年，一位名叫 Seppo Linnainmaa 的研究人员在他的硕士论文中介绍了一种自动高效计算所有梯度的技术。这个算法现在被称为*反向模式自动微分*（或简称*反向模式自动微分*）。通过网络的两次遍历（**一次前向，一次后向**），它能够计算神经网络中每个模型参数的误差梯度。换句话说，它可以找出如何调整每个连接权重和每个偏差以减少神经网络的误差。然后可以使用这些梯度执行梯度下降步骤。如果重复这个自动计算梯度和梯度下降步骤的过程，神经网络的误差将逐渐下降，直到最终达到最小值。这种反向模式自动微分和梯度下降的组合现在被称为<font color=red>*反向传播*</font>（或简称*反向传播*）。

- 它一次处理一个小批量（例如，每个包含 32 个实例），并多次遍历整个训练集。每次遍历称为*纪元*。
- 每个小批量通过输入层进入网络。然后，算法计算小批量中每个实例的第一个隐藏层中所有神经元的输出。结果传递到下一层，计算其输出并传递到下一层，依此类推，直到得到最后一层的输出，即输出层。这是*前向传递*：它与进行预测完全相同，只是所有中间结果都被保留，因为它们需要用于反向传递。
- 接下来，算法测量网络的输出误差（即，使用比较期望输出和网络实际输出的损失函数，并返回一些误差度量）。
- 然后计算每个输出偏差和每个连接到输出层的连接对误差的贡献。这是通过应用*链式法则*（可能是微积分中最基本的规则）进行分析的，使得这一步骤快速而精确。
- 然后，算法测量每个下一层中每个连接贡献的误差量，再次使用链式法则，向后工作直到达到输入层。正如前面解释的那样，这个反向传递有效地测量了网络中所有连接权重和偏差的误差梯度，通过网络向后传播误差梯度（因此算法的名称）。
- 最后，算法执行梯度下降步骤，调整网络中所有连接权重，使用刚刚计算的误差梯度。

参考文章：https://www.cnblogs.com/apachecn/p/18006162

## numpy dot versus matmul

```python
import numpy as np

# Define two vectors for the dot product
vector_a = np.array([2, 3, 4])
vector_b = np.array([1, 5, 6])

# Calculate the dot product
dot_result = np.dot(vector_a, vector_b)

print(f"Dot Product: {dot_result}")

# Define two matrices for matrix multiplication
matrix_a = np.array([[1, 2, 3], [4, 5, 6]])
matrix_b = np.array([[7, 8], [9, 10], [11, 12]])

# Perform matrix multiplication
matmul_result = np.matmul(matrix_a, matrix_b)

print("Matrix Multiplication:")
print(matmul_result)
```

result：

Dot Product: 41
Matrix Multiplication:
[[ 58  64]
 [139 154]]

They serve different purposes and are used for distinct mathematical operations. `np.dot()` calculates the dot product between two arrays, whereas `np.matmul()` is specifically designed for matrix multiplication.

## 正则化、偏差和方差

**正则化系数的选择**

选择正则化系数λ 的值时也需要思考欠拟合和过拟合的问题。不知道如何理解正规化

![img](https://i.loli.net/2018/12/01/5c0270d201ec5.png)

通常，我们选择一系列的想要测试的 λ 值，通常是 0-10 之间的呈现 2 倍关系的值（如：0,0.01,0.02,0.04,0.08,0.15,0.32,0.64,1.28,2.56,5.12,10 共 12 个）。 我们同样把数据分为训练集、交叉验证集和测试集。

之后利用公式（看不懂公式）：

![img](https://i.loli.net/2018/12/01/5c02716ba966d.png)

来选择λ，步骤是：

1. 使用训练集训练12个不同程度的正则化的模型
2. 用12个模型分别对交叉验证集计算出交叉验证误差，选择误差最小的模型

得出的结果图：https://momodel.github.io/mlbook/09/09-5.html

## 处理倾斜数据集

数据不平衡，比如处理不合格产品，合格率位96.4%，如果每次预测结果都是合格，准确率就可以达到96.4%，显然这种做法是不对的。

可以采用混淆矩阵来处理这种分类问题，对于给定的类别， 精确度和召回率的不同组合可以给出不同的意义，还可以使用ROC曲线来预测。

高质量回答：https://towardsdatascience.com/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28

## 决策树

Decision Tree是一种解决分类问题的算法，监督学习主要有两种任务：分类（连续）、回归（离散）。

决策树算法采用树结构，层层推理来实现最终的分类。

决策树学习的三个步骤：特征选择、决策树生成、决策树剪枝（解决过拟合问题，随机森林很大程度减少过拟合）。

三种典型的决策树算法：ID3（采用信息增益）、C4.5（改进、采用信息增益比）、CART（采用基尼系数，**CART树全称Classification And Regression Trees**）

**关于信息增益的理解：**

熵：表示随机变量的不确定性

条件熵：在一个条件下、随机变量的不确定性能

信息增益：熵-条件熵，表示是在一个条件下，信息不确定性减少的程度。

详细解释：https://www.zhihu.com/question/22104055

## One-Hot编码

独热编码，意思就是如果是相应编码位就是1，如果否相应编码位就是0，比如男女就可以用编码：01、10，再比如操作系统的采用位示图来表示磁盘中一个盘块的使用情况。

## 回归树

构建方式的核心：切分方式与属性选择

 递归二分法

https://www.showmeai.tech/article-detail/192

## How to understand clustering

这是一种非监督学习的算法（与classification最大的区别），将不同性质的数据分成几个相同类型的数据，如何评定相同类型的数据，可以有下面这些标准：Euclidean distance, Cosine similarity, Manhattan distance, etc.

聚类算法的分类：

1. Centroid-based Clustering (Partitioning methods)
2. Density-based Clustering (Model-based methods)
3. Connectivity-based Clustering (Hierarchical clustering)
4. Distribution-based Clustering

[Clustering in Machine Learning - GeeksforGeeks](https://www.geeksforgeeks.org/clustering-in-machine-learning/#what-is-clustering-)

## 异常检测算法

找到与数据集分布不一致的算法，离群点、异常值检测

一般针对无监督异常检测，因为异常点常常是没有标签的

无监督异常检测算法的思想：

1、基于聚类的算法

2、基于统计的算法

3、基于深度的方法

4、基于分类模型

5、基于偏差的方法

6、基于重构的算法

7、基于神经网络的算法

## 丑小鸭定理

丑小鸭定理（Ugly Duckling Theorem）是1969 年由渡边慧提出的[Watanabe,1969]．“丑小鸭与白天鹅之间的区别和两只白天鹅之间的区别一样大”．这个定理初看好像不符合常识，但是仔细思考后是非常有道理的．因为世界上不存在相似性的客观标准，一切相似性的标准都是主观的．如果从体型大小或外貌的角度来看，丑小鸭和白天鹅的区别大于两只白天鹅的区别；但是如果从基因的角度来看，丑小鸭与它父母的差别要小于它父母和其他白天鹅之间的差别．

这里的“丑小鸭”是指白天鹅的幼雏，而不是“丑陋的小鸭子”．渡边慧（1910～1993），美籍日本学者，理论物理学家，也是模式识别的最早研究者之一．

相似性的评定也需要主观标准，训练结果的相似性也要针对具体情况。

ps: 摘自 邱锡鹏，神经网络与深度学习，机械工业出版社，https://nndl.github.io/, 2020.

## 超参数

超参数（Hyperparameter）是机器学习和深度学习中需要在训练模型之前设置的参数，而不是通过模型训练自动学习的参数。它们影响模型的训练过程和性能表现，通常需要通过实验和调优来选择最佳值。

超参数和模型参数的区别是：

- **模型参数**：模型训练过程中从数据中学习得到的参数，例如线性回归中的权重或神经网络中的权重和偏置。
- **超参数**：在训练之前设定的参数，决定了模型的结构和训练过程，如学习率、批量大小（batch size）、隐藏层的数量等。

常见的超参数包括：

1. **学习率（Learning Rate）**：控制每次更新模型参数的步长。
2. **批量大小（Batch Size）**：每次更新模型时使用的样本数量。
3. **优化器类型（Optimizer）**：用于更新模型参数的方法，如SGD、Adam等。
4. **正则化参数（Regularization Parameter）**：控制模型复杂度的参数，用于防止过拟合。
5. **隐藏层数量和单元数量**：对于神经网络，定义每层隐藏层的神经元数量和隐藏层的个数。

调优超参数通常通过交叉验证或网格搜索等技术进行，以找到模型性能的最佳配置。

## 梯度下降算法

在具体使用梯度下降法的过程中，主要有以下几种不同的变种，即：batch、mini-batch、Stochastic。其主要区别是不同的变形在训练数据的选择上。

**批量梯度下降法**(Batch Gradient Descent)针对的是整个数据集，通过对所有的样本的计算来求解梯度的方向。在每次迭代时需要计算每个样本上损失函数的梯度并求和。当训练集中的样本数量𝑁 很大时，空间复杂度比较高，每次迭代的计算开销也很大。

mini-batch GD：在上述的批梯度的方式中每次迭代都要使用到所有的样本，对于数据量特别大的情况，如大规模的机器学习应用，每次迭代求解所有样本需要花费大量的计算成本。是否可以在每次的迭代过程中**利用部分样本代替所有的样本**呢？基于这样的思想，便出现了mini-batch的概念。

BGD和SGD的对比：**批量梯度下降法**相当于是从真实数据分布中采集𝑁 个样本，并由它们计算出来的经验风险的梯度来近似期望风险的梯度．为了减少每次迭代的计算复杂度，我们也可以在每次迭代时只采集一个样本，计算这个样本损失函数的梯度并更新参数，即**随机梯度下降法**。在**非凸优化问题**中，随机梯度下降更容易逃离局部最优点。

SGD也有一些缺点，比如可能会导致收敛不稳定和收敛速度较慢。因此，许多研究者和工程师会使用 SGD 的变种，如 **动量（Momentum）**、**Nesterov 加速梯度（Nesterov Accelerated Gradient）**、**Adam** 等优化器，来克服这些缺点。

**计算梯度的过程**：在神经网络中，反向模式、前向模式都是都是应用链式法则的梯度累计方式，反向模式是更为有效的计算模式。在计算时，前向传播时会有一个计算图保存着所有操作节点的及其之间的关系，这样在反向传播计算梯度时（即调用backwrad函数），PyTorch 从损失函数（通常是计算图的最后一个节点）开始，沿着计算图向后遍历。在每个节点，PyTorch 会计算当前节点的输出对输入节点的导数（即梯度），并将这些梯度累加到每个输入的 `.grad` 属性中。

对于复合函数 $$f(x; w,b)=\frac{1}{e^{-(wx+b)}+1}$$ 的计算图如下：

![计算图](https://s2.loli.net/2024/10/27/J5zfGqeyHURiSVo.png)

参考：邱锡鹏，神经网络与深度学习，机械工业出版社，https://nndl.github.io/, 2020.

Blog：[批梯度下降法(Batch Gradient Descent )，小批梯度下降 (Mini-Batch GD)，随机梯度下降 (Stochastic GD)](https://blog.csdn.net/cs24k1993/article/details/79120579)，里面有几张非常形象的图片

[SGD with Momentum](https://paperswithcode.com/method/sgd-with-momentum) 没看完

[深度学习笔记:详解优化器之随机梯度下降（SGD）](https://blog.csdn.net/weixin_53765658/article/details/136668114) 没看完

## 非凸优化问题

在优化问题中，目标是找到使目标函数达到最小值或最大值的变量集。凸优化问题涉及的函数具有凸性质，即函数图像上的两点连线在函数图像的上方。这种情况下，局部最小值也是全局最小值，优化问题相对较为简单。然而，非凸优化问题涉及的函数图像可能出现上下凹凸，存在多个局部极小/大值，从而使问题变得复杂而有趣。

## 神经网络的三个概念：Batch、Epoch、Iteration

Batch表示批次，Iteration表示迭代，Epoch表示一代

Batch：batch_size将影响到模型的优化程度，选择batch_size是为了在内存效率和内存容量之间进行权衡，

batch_size选择总结：过小，训练数据会难以收敛，从而导致underfitting；增大，相对处理速度加快，所需内存容量增加。所以需要权衡，找到一个合适的batch_size。

Epoch：是指将全部的样本（所以的batch）都完成一次forward+一次backward，通常情况下，一个epoch是不够的，在多个epoch后，模型的weight和bias会逐渐的更新，达到理想状态。  

Iteration：一个`iteration`包括了一个`step`中前向传播、损失计算、反向传播和参数更新的流程，有`batch_size`个iteration。

举个栗子：训练样本数量10000，batch_size=100，一共100个Batch，一个Batch有100个Iteration，一个Epoch有100个Barch的训练过程，但是在每个Batch都会去更新weight和bias。

参考：https://www.zhihu.com/question/43673341

## 梯度消失与梯度爆炸问题

### 问题产生的原因

循环神经网络（Recurrent Neural Network，RNN）是一类具有短期记忆能力的神经网络．在循环神经网络中，神经元不但可以接受其他神经元的信息，也可以接受自身的信息，形成具有环路的网络结构。

在RNN中，梯度计算时需要沿时间步反向传播（BPTT，Backpropagation Through Time），这个算法即按照时间的逆序将梯度信息一步步地往前传递．当输入序列比较长时了，时间步展开导致的**长链乘积**会存在梯度爆炸和消失问题，也称为长程依赖问题。

![简单的前馈神经网络](https://s2.loli.net/2024/11/06/53eNQXLIskyMFW6.png)

上图是一个简单的循环神经网络，只有一个隐藏层的神经网络，$$𝒉_𝑡$$ 不仅和当前时刻的输入$$𝒙_𝑡$$ 相关，也和上一个时刻的隐藏层状态$$𝒉_{𝑡−1} $$相关

![梯度消失与爆炸](https://s2.loli.net/2024/11/06/tJHswL5PTy1G6Oi.png)

### 分析步骤

![分析步骤](https://s2.loli.net/2024/11/06/THbBFXl3ghcQN5w.png)

分析：

1、使用了一个4层的前馈神经网络来模拟展开的RNN。

2、若激活函数是Sigmoid，求梯度的最大值是0.25，可能会导致梯度消失问题，这时候网络就学习不到东西了，即无法更新梯度。

3、若权重设置过大了，可能出现梯度爆炸问题，梯度变成NaN。

注：最后等式括号中根据不同的激活函数有不同的导数。

Blog:

https://medium.com/metaor-artificial-intelligence/the-exploding-and-vanishing-gradients-problem-in-time-series-6b87d558d22

https://www.linkedin.com/advice/3/how-do-you-deal-vanishing-exploding-gradient

https://www.cnblogs.com/XDU-Lakers/p/10557496.html

https://www.cnblogs.com/imreW/p/17366268.html

https://www.cnblogs.com/zf-blog/p/12793019.html

# 总结

主要将平时遇到的问题来记录，并且加以补充和整理，参考了多本书籍，如：邱锡鹏，神经网络与深度学习，机械工业出版社，https://nndl.github.io/, 2020.
以及 艾伯特深度学习 中文版[aibbt.com]等
对于许多的博客中比较好理解的内容也进行了摘录

实现以下算法：线性回归、Logistic 回归、神经网络、SVMs 、PCA 、K 均值算法。*[也许该用 python 重新实现一遍？]*